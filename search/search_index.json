{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Self-Learning Journey \ud83d\udcd8\u00b6","text":"<p>Welcome to my self-learning GitHub repository! This repo documents my journey of continuous learning and growth in various fields, including programming, algorithms, data structures, artificial intelligence, web development, and more.</p>"},{"location":"#table-of-contents","title":"Table of Contents\u00b6","text":"<ul> <li>Self-Learning Journey \ud83d\udcd8</li> <li>Table of Contents</li> <li>Overview</li> <li>Goals</li> <li>Learning Topics</li> <li>Project Structure</li> <li>Resources</li> <li>How to Use this Repo</li> <li>Future Plans</li> <li>Contributing</li> <li>License</li> </ul>"},{"location":"#overview","title":"Overview\u00b6","text":"<p>This repository serves as a collection of the exercises, notes, projects, and code snippets I\u2019ve worked on while learning various concepts and technologies. It\u2019s a living document that grows as I dive deeper into new topics and revisit previously learned material.</p>"},{"location":"#goals","title":"Goals\u00b6","text":"<ol> <li>Build a solid foundation in key programming concepts.</li> <li>Solve algorithmic problems to improve problem-solving skills.</li> <li>Develop hands-on projects to apply theoretical knowledge.</li> <li>Explore advanced topics such as AI, computer vision, and data science.</li> <li>Document my learning process and progress for future reference.</li> </ol>"},{"location":"#learning-topics","title":"Learning Topics\u00b6","text":"<p>Some of the key areas covered in this repo: - Programming Languages: JavaScript, Python, etc. - Algorithms and Data Structures: Arrays, Strings, Sliding Window, Dynamic Programming, etc. - Web Development: HTML, CSS, JavaScript, Frontend Frameworks, Backend Development, etc. - Artificial Intelligence: Machine Learning, Deep Learning, Computer Vision, etc. - Projects: Full-stack applications, data analysis tools, AI models, etc.</p>"},{"location":"#project-structure","title":"Project Structure\u00b6","text":"<p>The repository is organized as follows:</p> <pre><code>\u251c\u2500\u2500 docs\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 annotation-platform.md\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 github-jekyl.md\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 index.md\n\u251c\u2500\u2500 LICENSE\n\u251c\u2500\u2500 mkdocs.sh\n\u251c\u2500\u2500 mkdocs.yml\n\u251c\u2500\u2500 README.md\n\u2514\u2500\u2500 requirement.txt\n</code></pre> <p>Each folder contains relevant code snippets, problem solutions, and notes.</p>"},{"location":"#resources","title":"Resources\u00b6","text":"<p>Here are some of the resources I frequently refer to during my learning: - Books: [Insert book titles] - Online Courses: [Insert course links] - Websites: [Insert websites/blogs] - YouTube Channels: [Insert channels]</p>"},{"location":"#how-to-use-this-repo","title":"How to Use this Repo\u00b6","text":"<ul> <li>Browse through the folders based on topics of interest.</li> <li>Check out the projects folder for hands-on implementations.</li> <li>Refer to the notes folder for detailed explanations and documentation.</li> <li>Feel free to clone the repository and use it for your own learning!</li> </ul>"},{"location":"#future-plans","title":"Future Plans\u00b6","text":"<ul> <li>Dive deeper into advanced data structures and algorithms.</li> <li>Build more full-stack projects.</li> <li>Experiment with AI-based solutions in real-world applications.</li> <li>Expand the repository to include collaborative learning initiatives.</li> </ul>"},{"location":"#contributing","title":"Contributing\u00b6","text":"<p>This repository is primarily for personal learning, but contributions are welcome! If you find a better solution, have suggestions, or want to share your own learning path, feel free to submit a pull request.</p>"},{"location":"#license","title":"License\u00b6","text":"<p>This project is licensed under the MIT License - see the LICENSE file for details.</p>"},{"location":"nlp/","title":"Introdunction Natural Language Processing (NLP)\u00b6","text":""},{"location":"nlp/#prerequisites","title":"Prerequisites\u00b6","text":"<ul> <li>Python</li> <li>Basic Understanding of Statistics</li> <li>Basic Knowledge of Machine Learning</li> <li>Data Preprocessing</li> <li>ANN</li> <li>Optimization</li> </ul>"},{"location":"nlp/#what-is-nlp","title":"What is NLP?\u00b6","text":"<ul> <li>The goal of NLP is to develop algorithms and systems that allow machines to process and interact with natural languages, such as English, Spanish, or Mandarin, just like humans do.   Core NLP Tasks: Named Entity Recognition (NER): Identifying and classifying entities in text, such as names of people, places, or organizations.   Sentiment Analysis: Determining the emotional tone behind a series of words, often used to analyze opinions in text data.   Machine Translation: Automatically translating text from one language to another (e.g., Google Translate).   Speech Recognition: Converting spoken language into written text.   Text Summarization: Automatically generating concise summaries of longer texts.   Question Answering: Systems that can answer questions posed in natural language.</li> </ul> <p>Text Preprocessing:Preparing raw text data for analysis, which includes: Tokenization: Splitting text into smaller units, like words or sentences. Stemming/Lemmatization: Reducing words to their base or root form. Stop Word Removal: Eliminating common words like \"the\" or \"is\" that don\u2019t add much meaning to the analysis. POS Tagging: Identifying the part of speech (noun, verb, adjective) for each word in a sentence.</p>"},{"location":"nlp/#why-nlp","title":"Why NLP\u00b6","text":"<ul> <li> <p>Natural Language Processing (NLP) plays a crucial role in modern technology for several compelling reasons.</p> </li> <li> <p>Human-Machine Communication    Natural Language Interface: NLP allows people to interact with machines using natural language (like English, Spanish, etc.), making communication more intuitive compared to traditional commands or code.    Voice Assistants: NLP powers voice-activated systems like Siri, Alexa, and Google Assistant, enabling natural conversations and commands.    Customer Service Automation: Chatbots and virtual assistants, fueled by NLP, handle customer queries in a human-like manner, reducing the need for human intervention and improving efficiency.</p> </li> <li> <p>Text Data Analysis    Massive Text Data: The digital world generates vast amounts of unstructured text data\u2014emails, social media posts, reviews, articles, etc. NLP helps analyze and extract valuable insights from this data.    Sentiment Analysis: Businesses use NLP to understand public sentiment about products, services, or brands from social media, reviews, and surveys. This helps in gauging customer satisfaction and brand reputation.</p> </li> <li> <p>Improving Search Engines    Better Search Results: NLP enhances search engines by enabling them to understand natural language queries more accurately and provide more relevant results (e.g., Google Search, Bing).    Semantic Search: Rather than just matching keywords, NLP helps search engines understand the meaning behind the query, providing more accurate responses to complex questions.</p> </li> <li> <p>Document Processing and Automation    Automating Content: NLP helps automate the summarization of long documents, extraction of important information, or translation of content into multiple languages, saving time and reducing manual effort.    Legal and Financial Analysis: NLP is used to process complex legal, financial, and medical documents, making it easier for professionals to retrieve critical information without manually combing through pages.</p> </li> <li> <p>Personalization    Content Recommendations: NLP helps platforms like Netflix, YouTube, and Amazon recommend personalized content or products by understanding user preferences based on search history or past interactions.    Ad Targeting: NLP helps advertisers analyze user data to show more relevant ads based on user interests, making advertising more effective.</p> </li> <li> <p>Improving Accessibility    Assistive Technologies: NLP helps people with disabilities by enabling speech-to-text, text-to-speech, and real-time captioning. This improves access to technology for people who have visual, hearing, or mobility impairments.    Language Learning: NLP-based tools like Grammarly or Duolingo assist users in improving their language skills by providing grammar corrections, feedback, and language learning assistance.</p> </li> </ul>"},{"location":"nlp/#tokenization","title":"Tokenization\u00b6","text":"<ul> <li> <p>These tokens can be words, sentences, or subwords, depending on the level of tokenization. It is an essential preprocessing step that helps prepare text data for further analysis or for input into machine learning models.   Types of Tokenization:</p> </li> <li> <p>Word Tokenization:</p> </li> <li>In this process, the text is split into individual words.</li> <li>For example:<ul> <li>Input: \"Natural Language Processing is interesting.\"</li> <li>Output: [\"Natural\", \"Language\", \"Processing\", \"is\", \"interesting\", \".\"]</li> </ul> </li> <li>This is the most common form of tokenization, and it's useful for most NLP tasks.</li> <li>Sentence Tokenization:</li> <li>The text is divided into sentences rather than words.</li> <li>For example:<ul> <li>Input: \"I love NLP. It's very useful.\"</li> <li>Output: [\"I love NLP.\", \"It's very useful.\"]</li> </ul> </li> <li>Sentence tokenization is useful when the context of an entire sentence is important.</li> <li>Character Tokenization:</li> <li>This type of tokenization splits the text into individual characters.</li> <li> <p>For example:</p> <ul> <li>Input: \"NLP\"</li> <li>Output: [\"N\", \"L\", \"P\"]</li> </ul> </li> <li> <p>Character tokenization is useful in tasks like text generation or dealing with languages that don\u2019t have spaces between words (like Chinese or Japanese).</p> </li> <li> <p>Subword Tokenization:</p> </li> <li>Subword tokenization, words are split into subwords, typically using techniques like Byte Pair Encoding (BPE) or WordPiece.</li> <li>For example:<ul> <li>Input: \"unbelievable\"</li> <li>Output: [\"un\", \"believ\", \"able\"]</li> </ul> </li> <li> <p>This method is commonly used in modern NLP models (e.g., BERT, GPT) as it helps reduce vocabulary size while allowing the model to handle rare words more efficiently.</p> </li> <li> <p>NLTK (Natural Language Toolkit): A popular Python library that provides various tokenization methods for words and sentences.</p> </li> </ul> <pre><code>from nltk.tokenize import word_tokenize, sent_tokenize\ntext = \"Natural Language Processing is interesting. Let's learn it!\"\nword_tokens = word_tokenize(text)\nsent_tokens = sent_tokenize(text)\nprint(word_tokens)\nprint(sent_tokens)\n</code></pre> <ul> <li>spaCy: A fast NLP library that includes built-in tokenizers for different languages.</li> </ul> <pre><code>import spacy\nnlp = spacy.load(\"en_core_web_sm\")\ndoc = nlp(\"Natural Language Processing is amazing!\")\ntokens = [token.text for token in doc]\nprint(tokens)\n</code></pre> <ul> <li>Hugging Face's Tokenizers: A library for subword tokenization, particularly for transformer models like BERT and GPT.</li> </ul>"},{"location":"nlp/#stemminglemmatization","title":"Stemming/Lemmatization\u00b6","text":"<ul> <li> <p>Stemming and Lemmatization are two key techniques in Natural Language Processing (NLP) for text normalization. Both aim to reduce words to their base or root form, but they do so in different ways.</p> </li> <li> <p>Stemming:</p> </li> <li> <p>Stemming is the process of reducing a word to its base or root form by chopping off prefixes or suffixes. The result may not be a valid word but is intended to represent the \"stem\" of the word.</p> </li> <li>Example: - \"running\" \u2192 \"run\", \"runner\" \u2192 \"run\", \"studies\" \u2192 \"studi\"</li> <li>Stemming algorithms (like Porter Stemmer) apply rules to remove common suffixes such as \"ing\", \"ed\", \"es\", \"s\", etc., without considering the actual meaning of the word. - Pros - Fast and efficient. - Useful in applications where approximate matches are sufficient (e.g., search engines). - Cons - The output is often not a valid word (e.g., \"studies\" becomes \"studi\"). - Can lead to inaccurate results, as it doesn't account for the context or meaning of the word.</li> <li>Example</li> </ul> <pre><code>  from nltk.stem import PorterStemmer\n  stemmer = PorterStemmer()\n  print(stemmer.stem(\"running\")) # Output: run\n  print(stemmer.stem(\"studies\")) # Output: studi\n</code></pre> <ol> <li> <p>Lemmatization</p> </li> <li> <p>Lemmatization is the process of reducing a word to its base or root form, known as the lemma, while considering its meaning and part of speech. Lemmatization returns a valid word.</p> </li> <li>Example:</li> <li>\"running\" \u2192 \"run\", \"better\" \u2192 \"good\", \"studies\" \u2192 \"study\"</li> <li>Lemmatization uses a dictionary and considers the context, such as whether the word is a noun, verb, or adjective. This process ensures that the lemma is a valid word.</li> <li>Pros</li> <li>More accurate than stemming.</li> <li>Always produces a valid word.</li> <li>Cons:</li> <li>Slower than stemming due to the need for dictionary lookup and context analysis.</li> <li>Example</li> </ol> <pre><code>  from nltk.stem import WordNetLemmatizer\n  lemmatizer = WordNetLemmatizer()\n  print(lemmatizer.lemmatize(\"running\", pos=\"v\"))  # Output: run\n  print(lemmatizer.lemmatize(\"better\", pos=\"a\"))   # Output: good\n</code></pre> <ul> <li>When to Use:</li> </ul> <p>Stemming: Use when you need a fast and approximate solution, like in search engines or for applications where speed is more important than perfect accuracy.</p> <p>Lemmatization: Use when you need accuracy, and the exact meaning of words is critical, like in sentiment analysis or text classification tasks.</p>"},{"location":"nlp/#why-remove-stop-words","title":"Why Remove Stop Words?\u00b6","text":"<ul> <li>Reduce Noise: Stop words often add unnecessary noise to text data, making it harder to focus on important words. For example, in the sentence \"The cat is on the mat,\" the key words are \"cat\" and \"mat\", while \"the\", \"is\", \"on\" are considered irrelevant.</li> <li>Efficiency: Removing stop words reduces the number of tokens that need to be processed, which can speed up computations, especially in large datasets.</li> <li>Improve Model Performance: For many NLP tasks, removing stop words can enhance model performance by focusing the model on the most meaningful words.</li> <li>Dimensionality Reduction: In tasks like text classification or sentiment analysis, stop word removal helps reduce the dimensionality of the text, making the data easier to handle.</li> <li>Example <pre><code>import nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\nnltk.download('stopwords')\nnltk.download('punkt')\n\nstop_words = set(stopwords.words('english'))\ntext = \"The quick brown fox jumps over the lazy dog.\"\nwords = word_tokenize(text)\n\nfiltered_sentence = [w for w in words if not w.lower() in stop_words]\nprint(filtered_sentence)\n</code></pre></li> </ul> <pre><code>import spacy\n\nnlp = spacy.load('en_core_web_sm')\ndoc = nlp(\"The quick brown fox jumps over the lazy dog.\")\n\nfiltered_sentence = [token.text for token in doc if not token.is_stop]\nprint(filtered_sentence)\n</code></pre> <pre><code>from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n\ntext = \"The quick brown fox jumps over the lazy dog.\"\nwords = text.split()\nfiltered_sentence = [word for word in words if word.lower() not in ENGLISH_STOP_WORDS]\nprint(filtered_sentence)\n</code></pre> <pre><code>custom_stop_words = stop_words.union({\"fox\", \"dog\"})\nfiltered_sentence = [w for w in words if not w.lower() in custom_stop_words]\nprint(filtered_sentence)\n</code></pre>"},{"location":"nlp/#terminologies-used-in-nlp","title":"Terminologies used in NLP\u00b6","text":"<ol> <li>CORPUS</li> <li>Documents</li> <li>Vocabulary</li> <li>Words</li> </ol>"},{"location":"nlp/bow/","title":"Bow","text":""},{"location":"nlp/bow/#bag-of-words-bow","title":"Bag of Words (BoW)\u00b6","text":"<ul> <li>Bag of Words (BoW) is a simple and widely used technique in Natural Language Processing (NLP) to represent text data as numerical features for machine learning models. </li> <li>The BoW model converts text into a \"bag\" of individual words, ignoring grammar and word order but preserving the frequency of words.</li> </ul>"},{"location":"nlp/bow/#key-concepts-of-bag-of-words","title":"Key Concepts of Bag of Words:\u00b6","text":"<p>Vocabulary: BoW creates a set of all unique words (or \"tokens\") from the text data, which forms the vocabulary.</p> <p>Word Frequency: Each word in the text is assigned a frequency count, representing how many times it appears in the document.</p> <p>Document Representation: Each document (or sentence) is represented as a vector, where each dimension corresponds to a word from the vocabulary, and the value represents the frequency of that word in the document.</p>"},{"location":"nlp/bow/#how-bow-works","title":"How BoW Works\u00b6","text":"<ol> <li> <p>Tokenization: First, the text is split into individual tokens (words or phrases).</p> </li> <li> <p>Vocabulary Building: A unique vocabulary of words from the entire corpus is constructed.</p> </li> <li> <p>Vector Representation: For each document, a vector is created, where each position corresponds to a word in the vocabulary, and the value in that position is the count of the word in the document. If the word does not appear, the count is 0.</p> </li> </ol>"},{"location":"nlp/bow/#example-of-bag-of-words","title":"Example of Bag of Words\u00b6","text":"<ul> <li>Let\u2019s assume we have two sentences:</li> <li>Sentence 1: \"The cat sat on the mat.\"</li> <li>Sentence 2: \"The dog lay on the rug.\"</li> </ul> <p>Step 1: Tokenization</p> <ul> <li>Extract the words (ignoring punctuation):</li> <li>Sentence 1: ['the', 'cat', 'sat', 'on', 'the', 'mat']</li> <li>Sentence 2: ['the', 'dog', 'lay', 'on', 'the', 'rug']</li> </ul> <p>Step 2: Vocabulary Building</p> <ul> <li>Create a unique list of words (vocabulary) from both sentences:</li> <li>Vocabulary: ['the', 'cat', 'sat', 'on', 'mat', 'dog', 'lay', 'rug']</li> </ul> <p>Step 3: Vector Representation</p> <ul> <li>Create a frequency vector for each sentence based on the vocabulary:</li> <li>Sentence 1: [2, 1, 1, 1, 1, 0, 0, 0]</li> <li>'the' appears twice, 'cat', 'sat', 'on', and 'mat' appear once, and 'dog', 'lay', 'rug' do not appear.</li> <li>Sentence 2: [2, 0, 0, 1, 0, 1, 1, 1]</li> <li>'the' appears twice, 'dog', 'lay', 'on', and 'rug' appear once, and 'cat', 'sat', 'mat' do not appear.</li> </ul>"},{"location":"nlp/bow/#advantages-of-bow","title":"Advantages of BoW\u00b6","text":"<ol> <li>Simplicity: BoW is easy to understand and implement.</li> <li>Effective for Simple Tasks: It works well for tasks where word frequency is important, like document classification or spam detection.</li> </ol>"},{"location":"nlp/bow/#limitations-of-bow","title":"Limitations of BoW\u00b6","text":"<ol> <li>Ignores Word Order: BoW disregards the order of words in the document, so it cannot capture the meaning or context of words based on how they are arranged.<ul> <li>Example: The sentences \"The dog chased the cat\" and \"The cat chased the dog\" would have the same BoW representation, despite having different meanings.</li> </ul> </li> <li>High Dimensionality: For large corpora, the vocabulary can become very large, resulting in high-dimensional feature vectors that may be inefficient to process.</li> <li>Sparse Representation: Most BoW vectors are sparse, meaning that many positions in the vector are 0, which can be inefficient in terms of memory and computation.</li> </ol>"},{"location":"nlp/bow/#variations-of-bow","title":"Variations of BoW\u00b6","text":"<ol> <li>TF-IDF (Term Frequency-Inverse Document Frequency): Instead of using raw word counts, TF-IDF assigns weights to words based on their importance within the document and across all documents in the corpus. This reduces the importance of common words and highlights words that are more meaningful in specific contexts.</li> <li>N-grams: BoW typically treats each word as an independent token, but in the N-gram model, contiguous sequences of N words (bigrams, trigrams, etc.) are considered as tokens to preserve some word order.</li> </ol>"},{"location":"nlp/bow/#implementing-bow-in-python","title":"Implementing BoW in Python\u00b6","text":"<p><pre><code>from sklearn.feature_extraction.text import CountVectorizer\n\n# Sample sentences\ndocuments = [\"The cat sat on the mat.\", \"The dog lay on the rug.\"]\n\n# Create a Bag of Words model\nvectorizer = CountVectorizer()\n\n# Fit and transform the documents\nbow_matrix = vectorizer.fit_transform(documents)\n\n# View the vocabulary\nprint(vectorizer.get_feature_names_out())\n\n# View the Bag of Words representation\nprint(bow_matrix.toarray())\n</code></pre> <pre><code>import nltk\nfrom nltk.tokenize import word_tokenize\nfrom collections import Counter\n\n# Tokenize sentences\nsentence1 = \"The cat sat on the mat.\"\nsentence2 = \"The dog lay on the rug.\"\n\n# Tokenize and count word frequencies\ntokens1 = word_tokenize(sentence1.lower())\ntokens2 = word_tokenize(sentence2.lower())\n\nbow1 = Counter(tokens1)\nbow2 = Counter(tokens2)\n\nprint(bow1)\nprint(bow2)\n</code></pre></p>"},{"location":"nlp/tf-idf/","title":"TF-IDF","text":""},{"location":"nlp/tf-idf/#tf-idf-term-frequency-inverse-document-frequency","title":"TF-IDF (Term Frequency-Inverse Document Frequency)\u00b6","text":"<ul> <li>TF-IDF (Term Frequency-Inverse Document Frequency) is a statistical technique used in Natural Language Processing (NLP) to evaluate the importance of a word in a document relative to a collection of documents (called the corpus).</li> <li>Unlike simple word count or Bag of Words (BoW), TF-IDF not only considers how often a word appears in a document but also accounts for how common or rare the word is across the entire corpus.</li> <li>This helps identify the most relevant words while minimizing the impact of commonly occurring words (like \"the\", \"is\", \"and\").</li> </ul>"},{"location":"nlp/tf-idf/#key-components-of-tf-idf","title":"Key Components of TF-IDF\u00b6","text":"<ol> <li> <p>Term Frequency (TF):</p> <ul> <li>This measures how often a term (word) appears in a document. A higher term frequency means that the word is more important in that specific document.</li> <li>Formula:</li> <li>TF(t,d)= (Total\u00a0number\u00a0of\u00a0terms\u00a0in\u00a0document\u00a0d)/(Number\u00a0of\u00a0times\u00a0term\u00a0t\u00a0appears\u00a0in\u00a0document\u00a0d)</li> <li>Example: If the word \"cat\" appears 3 times in a document with 100 words, the TF for \"cat\" would be:</li> <li>TF(cat,document)= 100/3 = 0.03</li> </ul> </li> <li> <p>Inverse Document Frequency (IDF):</p> <ul> <li>This measures how important a word is across the entire corpus. Words that appear in many documents get a lower score, while words that appear in fewer documents get a higher score.</li> <li>Formula:</li> <li>IDF(t,D)=log(Total\u00a0number\u00a0of\u00a0documents/Number\u00a0of\u00a0documents\u00a0containing\u00a0the\u00a0term\u00a0t)</li> <li>Example: If the corpus contains 1000 documents, and the word \"cat\" appears in 50 of them, the IDF for \"cat\" would be:</li> <li>IDF(cat,corpus)=log( 1000/50)=log(20)\u22481.3</li> </ul> </li> <li> <p>TF-IDF Score:</p> <ul> <li>F-IDF combines the term frequency (TF) and inverse document frequency (IDF) to assign a weight to each word in a document. Words that are frequent in a document but rare in the corpus receive a higher score, indicating their importance.</li> <li>Formula:</li> <li>TF-IDF(t,d,D)=TF(t,d)\u00d7IDF(t,D)</li> <li>Example: Using the previous values for TF and IDF of the word \"cat\", the TF-IDF score for \"cat\" in a document would be:</li> <li>TF-IDF(cat)=0.03\u00d71.3=0.039</li> </ul> </li> </ol>"},{"location":"nlp/tf-idf/#example-of-tf-idf","title":"Example of TF-IDF\u00b6","text":"<ul> <li>Let's calculate TF-IDF for two simple documents:</li> <li>Document 1: \"The cat is on the mat.\"</li> <li>Document 2: \"The dog sat on the rug.\"</li> </ul> <p>Step 1: Calculate Term Frequency (TF)</p> <ul> <li>Document 1: \"The cat is on the mat.\" (Total words: 6)<ul> <li>TF(\"the\") = 2/6 = 0.33</li> <li>TF(\"cat\") = 1/6 = 0.17</li> <li>TF(\"mat\") = 1/6 = 0.17</li> <li>TF(\"is\") = 1/6 = 0.17</li> <li>TF(\"on\") = 1/6 = 0.17</li> </ul> </li> <li>Document 2: \"The dog sat on the rug.\" (Total words: 6)<ul> <li>TF(\"the\") = 2/6 = 0.33</li> <li>TF(\"dog\") = 1/6 = 0.17</li> <li>TF(\"sat\") = 1/6 = 0.17</li> <li>TF(\"on\") = 1/6 = 0.17</li> <li>TF(\"rug\") = 1/6 = 0.17</li> </ul> </li> </ul> <p>Step 2: Calculate Inverse Document Frequency (IDF)</p> <ul> <li>For a corpus of two documents:</li> <li>IDF(\"the\"): Appears in both documents, so:</li> <li>IDF(the)=log(2/2)=log(1)=0</li> <li> <p>(Common words like \"the\" get an IDF of 0, meaning they have no distinguishing power.)</p> </li> <li> <p>IDF(\"cat\"): Appears in Document 1 only:</p> </li> <li>IDF(cat)=log(2/1)=log(2)\u22480.693</li> <li>IDF(\"on\"): Appears in both documents:<ul> <li>IDF(on)=log(2/2)=0</li> </ul> </li> </ul> <p>Step 3: Calculate TF-IDF Scores</p> <ul> <li>For Document 1:<ul> <li>TF-IDF(\"cat\") = 0.17 \u00d7 0.693 = 0.118</li> <li>TF-IDF(\"the\") = 0.33 \u00d7 0 = 0</li> <li>TF-IDF(\"on\") = 0.17 \u00d7 0 = 0</li> </ul> </li> <li> <p>For Document 2:</p> <ul> <li>TF-IDF(\"dog\") = 0.17 \u00d7 0.693 = 0.118</li> <li>TF-IDF(\"the\") = 0.33 \u00d7 0 = 0</li> <li>TF-IDF(\"on\") = 0.17 \u00d7 0 = 0</li> </ul> </li> <li> <p>Thus, terms like \"cat\" and \"dog\" get a higher TF-IDF score because they are less frequent across the entire corpus, while common words like \"the\" and \"on\" get lower scores.</p> </li> </ul>"},{"location":"nlp/tf-idf/#advantages-of-tf-idf","title":"Advantages of TF-IDF:\u00b6","text":"<ul> <li>Relevance Weighting: TF-IDF assigns higher scores to words that are more important in a specific document while downplaying the impact of common words that appear frequently across documents.</li> <li>Effective for Document Search: TF-IDF is widely used in search engines to rank documents based on relevance to a query.</li> <li>Simplicity: It\u2019s simple to compute and works well as a baseline for text representation.</li> </ul>"},{"location":"nlp/tf-idf/#limitations-of-tf-idf","title":"Limitations of TF-IDF:\u00b6","text":"<ul> <li>Does Not Capture Semantics: TF-IDF only considers word frequency and does not capture the meaning or context of the words.</li> <li>Sparsity: For large vocabularies, the document-term matrix generated by TF-IDF can be sparse (lots of zeros), leading to inefficiencies in memory and computation.</li> <li>Fixed Vocabulary: TF-IDF requires a fixed vocabulary from the training corpus, making it less suitable for dynamic environments where new words are frequently introduced.</li> </ul>"},{"location":"nlp/tf-idf/#implementing-tf-idf-in-python","title":"Implementing TF-IDF in Python:\u00b6","text":"<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer\n\n# Sample documents\ndocuments = [\"The cat is on the mat.\", \"The dog sat on the rug.\"]\n\n# Initialize the TF-IDF Vectorizer\nvectorizer = TfidfVectorizer()\n\n# Fit and transform the documents\ntfidf_matrix = vectorizer.fit_transform(documents)\n\n# Display the vocabulary\nprint(vectorizer.get_feature_names_out())\n\n# Display the TF-IDF representation\nprint(tfidf_matrix.toarray())\n</code></pre> <pre><code>import nltk\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Download NLTK stop words (if you don't have them already)\nnltk.download('stopwords')\n\nfrom nltk.corpus import stopwords\n\n# Sample documents\ndocuments = [\n    \"The cat is on the mat.\",\n    \"The dog sat on the rug.\",\n    \"Cats and dogs are great pets.\"\n]\n\n# Load English stop words\nstop_words = stopwords.words('english')\n\n# Initialize the TF-IDF Vectorizer\nvectorizer = TfidfVectorizer(stop_words=stop_words)\n\n# Fit the model and transform the documents into TF-IDF representation\ntfidf_matrix = vectorizer.fit_transform(documents)\n\n# Display the vocabulary (unique terms)\nprint(\"Vocabulary:\", vectorizer.get_feature_names_out())\n\n# Display the TF-IDF values for each document\nprint(\"\\nTF-IDF Matrix:\")\nprint(tfidf_matrix.toarray())\n\n# Optionally: inspect the matrix in a readable way\nimport pandas as pd\n\n# Create a DataFrame for better visualization\ntfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\nprint(\"\\nTF-IDF DataFrame:\")\nprint(tfidf_df)\n</code></pre>"}]}